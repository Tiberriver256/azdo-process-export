{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Repository and Screaming Architecture",
        "description": "Create the project repository with a Screaming Architecture folder structure, separating domain logic from infrastructure details.",
        "details": "Use Python 3.11+ for best async support. Structure folders as /domain, /infrastructure, /cli, /tests, /features. Add pyproject.toml for dependency management (prefer Poetry). Include .gitignore and README.md. Follow Screaming Architecture principles: domain objects and logic in /domain, API clients in /infrastructure, CLI entrypoint in /cli.",
        "testStrategy": "Verify folder structure matches Screaming Architecture. Ensure all modules are importable and initial test runner executes.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Project Repository and Initialize Git",
            "description": "Set up a new project repository and initialize it with Git version control.",
            "dependencies": [],
            "details": "Create a new directory for the project, initialize Git, and set up the initial commit. Add a .gitignore file tailored for Python projects.",
            "status": "done",
            "testStrategy": "Verify that the repository is initialized, .gitignore is present, and Git status shows a clean working tree."
          },
          {
            "id": 2,
            "title": "Establish Screaming Architecture Folder Structure",
            "description": "Create the required folder structure following Screaming Architecture principles.",
            "dependencies": [],
            "details": "Add /domain, /infrastructure, /cli, /tests, and /features folders. Ensure /domain is for domain logic, /infrastructure for API clients and utilities, /cli for CLI entrypoint, /tests for unit tests, and /features for Behave scenarios.",
            "status": "done",
            "testStrategy": "Check that all folders exist and match the prescribed structure. Confirm that each folder is empty or contains a placeholder __init__.py."
          },
          {
            "id": 3,
            "title": "Configure Dependency Management with Poetry",
            "description": "Set up pyproject.toml using Poetry for dependency management and Python version specification.",
            "dependencies": [],
            "details": "Initialize Poetry in the project root, specify Python 3.11+ in pyproject.toml, and add initial dependencies as needed. Ensure pyproject.toml is present and correctly configured.",
            "status": "done",
            "testStrategy": "Run 'poetry install' to verify dependencies are installed and Python version is enforced."
          },
          {
            "id": 4,
            "title": "Add Project Documentation and Metadata Files",
            "description": "Create README.md and ensure project metadata files are present.",
            "dependencies": [],
            "details": "Write a README.md describing the project, architecture, and setup instructions. Ensure .gitignore and pyproject.toml are included in the repository.",
            "status": "done",
            "testStrategy": "Verify README.md is informative and all metadata files are present in the repository root."
          },
          {
            "id": 5,
            "title": "Validate Initial Importability and Test Runner Setup",
            "description": "Ensure all modules are importable and set up an initial test runner.",
            "dependencies": [],
            "details": "Add __init__.py files as needed. Set up a basic test runner (e.g., pytest) in /tests. Confirm that all modules in /domain, /infrastructure, and /cli can be imported without errors.",
            "status": "done",
            "testStrategy": "Run the test runner to confirm it executes and imports all modules successfully."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement CLI Entry Point with Click",
        "description": "Develop the CLI interface using Click, supporting the 'process' verb and all required options, following TDD principles.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "All CLI development must be TDD-driven. For each CLI feature, begin by documenting the desired behavior in a Behave scenario in /tests/features/cli_basic.feature. Write a failing test in /tests/steps/cli_steps.py, then implement the CLI code in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py until the test passes. Refactor only after the test passes. Use Click v8.1+ for robust CLI parsing. Implement 'process' command with options: --project, --out, --pat, --log-level, --skip-metrics, --version, --help. Ensure auto-generated help text matches PRD. Document this workflow and ensure all code changes are test-driven.",
        "testStrategy": "For each CLI feature, write a Behave scenario describing expected CLI invocation and output in /tests/features/cli_basic.feature. Implement failing step definitions in /tests/steps/cli_steps.py. Develop CLI code in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py until tests pass. Unit test CLI parsing and option handling. Refactor only after passing tests.",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up CLI Project Structure for TDD",
            "description": "Create the initial CLI project structure and ensure TDD workflow is established.",
            "status": "done",
            "dependencies": [],
            "details": "Create /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py. Set up /tests/features/cli_basic.feature and /tests/steps/cli_steps.py for Behave-driven development. Ensure CLI entry point is ready for TDD-based implementation.",
            "testStrategy": "Verify that the CLI entry point runs without errors and displays a basic help message. Confirm that Behave scenarios can be executed and fail as expected before implementation."
          },
          {
            "id": 2,
            "title": "Implement 'process' Command with Required Options via TDD",
            "description": "Develop the 'process' command in Click using TDD: start with Behave scenario, failing test, then CLI code.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Document the expected behavior for the 'process' command and its options (--project, --out, --pat, --log-level, --skip-metrics) in /tests/features/cli_basic.feature. Write failing step definitions in /tests/steps/cli_steps.py. Implement the command in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py until tests pass.",
            "testStrategy": "Unit test option parsing and validation for all supported flags. Behave scenario must pass for CLI invocation and option handling."
          },
          {
            "id": 3,
            "title": "Add Global Options and Version/Help Support via TDD",
            "description": "Integrate --version and --help options using TDD: scenario, failing test, then implementation.",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "Document expected help and version output in /tests/features/cli_basic.feature. Write failing step definitions in /tests/steps/cli_steps.py. Implement global options in CLI code until tests pass. Ensure help/version output matches PRD.",
            "testStrategy": "Compare CLI help and version output against PRD; Behave scenarios must pass for help/version invocation."
          },
          {
            "id": 4,
            "title": "Validate Option Handling and Error Messaging via TDD",
            "description": "Implement robust error handling for missing or invalid options using TDD: scenario, failing test, then implementation.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Document error cases and expected messages in /tests/features/cli_basic.feature. Write failing step definitions in /tests/steps/cli_steps.py. Implement error handling in CLI code until tests pass.",
            "testStrategy": "Unit test error cases and verify error messages match PRD expectations. Behave scenarios must pass for error handling."
          },
          {
            "id": 5,
            "title": "Integrate CLI with Behave and Unit Tests via TDD",
            "description": "Write Behave scenarios and unit tests to validate CLI invocation, option handling, and output structure, following TDD.",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "Ensure comprehensive test coverage for CLI behaviors, including all supported options and error cases. All new features must begin with a Behave scenario and failing test before implementation.",
            "testStrategy": "Implement Behave scenarios for CLI usage; write unit tests for option parsing and output. All tests must pass before refactoring."
          },
          {
            "id": 6,
            "title": "Document TDD Workflow for CLI Development",
            "description": "Create documentation outlining the TDD workflow for CLI development, including feature file creation, failing test writing, and implementation steps.",
            "status": "done",
            "dependencies": [],
            "details": "Write a README or developer guide describing the TDD process for CLI features: start with a Behave scenario in /tests/features/cli_basic.feature, write failing step definitions in /tests/steps/cli_steps.py, implement CLI code in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py, and refactor only after tests pass.",
            "testStrategy": "Review documentation for clarity and completeness. Ensure it covers all required TDD steps and references correct file locations."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Authentication Logic",
        "description": "Support DefaultAzureCredential chain and PAT override for Azure DevOps and OData APIs, following latest best practices for credential precedence, error handling, and logging.",
        "status": "pending",
        "dependencies": [
          2
        ],
        "priority": "high",
        "details": "Implement authentication logic in /infrastructure/auth.py. If --pat is provided, always use it for Basic Auth and do not fallback to Azure AD if PAT fails. If no PAT is provided, use DefaultAzureCredential from azure-identity==1.14.0 for Bearer token authentication. Ensure correct token scope for both Azure DevOps and OData endpoints. Centralize error handling and structured JSON logging for all credential operations and failures. Emit clear, actionable error messages and map fatal credential errors to exit code 2. Document credential precedence and troubleshooting steps in README and CLI help.",
        "testStrategy": "Use Behave scenarios to test both credential paths (PAT and Azure AD), including mocked failures for each. Verify error messaging, exit codes, and structured JSON logging output. Ensure correct token scopes are used for Azure DevOps and OData endpoints. Validate documentation updates for credential precedence and troubleshooting.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for basic PAT authentication",
            "description": "Create a Behave scenario testing basic PAT authentication and verify it fails initially.",
            "details": "<info added on 2025-07-18T21:28:17.526Z>\nBehave scenario for basic PAT authentication is present in /tests/features/authentication.feature. Missing step definitions for structured log and authentication header checks have been added to /tests/steps/cli_steps.py; these steps currently fail as expected, confirming the scenario is initially red. Next, run Behave to verify the test fails, then begin implementing the authentication logic to make the scenario pass.\n</info added on 2025-07-18T21:28:17.526Z>\n<info added on 2025-07-18T21:28:40.731Z>\nUpdate Behave configuration to recognize the custom steps directory by setting steps_dir in behave.ini or command-line options, or move/copy the step definitions from /tests/steps/cli_steps.py to /features/steps/cli_steps.py so Behave can discover them. After resolving the steps directory issue, rerun Behave to confirm the scenario fails as expected, ensuring the test setup is correct before proceeding to implement authentication logic.\n</info added on 2025-07-18T21:28:40.731Z>\n<info added on 2025-07-18T21:29:22.134Z>\nImplement the missing step definitions for the PAT authentication scenario, including 'Given I have a valid Personal Access Token', in /features/steps/cli_steps.py. Once all steps are defined and the scenario is recognized by Behave, proceed to implement minimal PAT authentication logic in /infrastructure/auth.py to make the test pass. This should include accepting a PAT via CLI, using it for Basic Auth, and ensuring the authentication header is set correctly. Run Behave to confirm the scenario passes, verifying both the step definitions and the initial authentication implementation.\n</info added on 2025-07-18T21:29:22.134Z>\n<info added on 2025-07-18T21:31:11.268Z>\nMigrate environment.py from /tests to /features to align with Behave's standard directory structure. Move all .feature files to the root of /features and consolidate step definitions in /features/steps. Ensure all BDD test files, including environment.py and step definitions, are located within the /features directory for proper Behave discovery and execution. Update any relevant Behave configuration or CI scripts to reference the new /features structure.\n</info added on 2025-07-18T21:31:11.268Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 2,
            "title": "Implement 1: Make basic PAT authentication scenario pass",
            "description": "Implement minimal PAT authentication in /infrastructure/auth.py to make the basic PAT scenario pass.",
            "details": "<info added on 2025-07-18T22:04:15.523Z>\nMinimal PAT authentication has been implemented in the CLI: the process command now invokes get_auth_headers(pat), logs authentication success or failure, and exits with code 0 on success or 2 on failure. The Behave scenario for PAT authentication is integrated with the CLI logic. Behave was executed, but all scenarios were skipped, likely due to missing or misconfigured tags or step definitions. No code errors were found in the CLI or authentication modules. Next step is to debug Behave configuration to ensure scenarios run and validate that the test passes.\n</info added on 2025-07-18T22:04:15.523Z>\n<info added on 2025-07-18T22:05:27.318Z>\nUpdate the CLI to emit a structured JSON log message when authentication succeeds using a PAT credential. The log event should include the correct event name (e.g., \"authentication_success\"), the credential source (\"PAT\"), and any other fields required by the Behave test expectation. Ensure the log format matches the test's requirements so the scenario step \"And the structured log should contain authentication success with PAT credential source\" passes.\n</info added on 2025-07-18T22:05:27.318Z>",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 3
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up PAT authentication implementation",
            "description": "Refactor PAT authentication implementation for clarity and maintainability while keeping tests green.",
            "details": "",
            "status": "done",
            "dependencies": [
              2
            ],
            "parentTaskId": 3
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for DefaultAzureCredential fallback",
            "description": "Create a Behave scenario testing DefaultAzureCredential fallback when no PAT is provided and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              3
            ],
            "parentTaskId": 3
          },
          {
            "id": 5,
            "title": "Implement 2: Make DefaultAzureCredential scenario pass",
            "description": "Implement DefaultAzureCredential fallback with correct Azure DevOps token scope to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              4
            ],
            "parentTaskId": 3
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up DefaultAzureCredential implementation",
            "description": "Refactor DefaultAzureCredential implementation for better structure while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              5
            ],
            "parentTaskId": 3
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for error handling and logging",
            "description": "Create a Behave scenario testing authentication error handling and structured JSON logging and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              6
            ],
            "parentTaskId": 3
          },
          {
            "id": 8,
            "title": "Implement 3: Make error handling and logging scenario pass",
            "description": "Implement structured JSON logging and centralized error handling with clear error messages to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              7
            ],
            "parentTaskId": 3
          },
          {
            "id": 9,
            "title": "Scenario 4: Write failing test for documentation and help",
            "description": "Create a Behave scenario testing credential precedence documentation and troubleshooting and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              8
            ],
            "parentTaskId": 3
          },
          {
            "id": 10,
            "title": "Implement 4: Make documentation scenario pass and final refactor",
            "description": "Implement documentation updates for README and CLI help to make the scenario pass, then final refactor for production readiness.",
            "details": "",
            "status": "pending",
            "dependencies": [
              9
            ],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Structured JSON Logging",
        "description": "Add structured logging per Better Stack guidance, supporting info/debug/trace levels and JSON output.",
        "details": "Use structlog==23.2.0 for JSON logging. No root logger; configure per module. Support --log-level option. Include timestamps, levels, and trace context. Output logs to stdout and optionally to file. Place logging config in /infrastructure/logging.py.",
        "testStrategy": "Unit test log output format. Behave scenarios for log-level switching. Validate logs are JSON and contain required fields.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for basic structured logging setup",
            "description": "Create a Behave scenario that tests basic structured JSON logger initialization and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Implement 1: Make basic logging scenario pass",
            "description": "Implement minimal code to make the basic structured logging scenario pass - create logger configuration and basic JSON output.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up basic logging implementation",
            "description": "Clean up the basic logging implementation for clarity and maintainability while ensuring tests remain green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 4
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for log level configuration",
            "description": "Create a Behave scenario testing different log levels (info, debug, trace) and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              3
            ],
            "parentTaskId": 4
          },
          {
            "id": 5,
            "title": "Implement 2: Make log level scenario pass",
            "description": "Implement log level configuration support to make the log level scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              4
            ],
            "parentTaskId": 4
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up log level implementation",
            "description": "Refactor log level implementation for better structure and maintainability while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              5
            ],
            "parentTaskId": 4
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for Better Stack compliance",
            "description": "Create a Behave scenario testing Better Stack guidance compliance for structured logging and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              6
            ],
            "parentTaskId": 4
          },
          {
            "id": 8,
            "title": "Implement 3: Make Better Stack compliance scenario pass",
            "description": "Implement Better Stack compliance features to make the compliance scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              7
            ],
            "parentTaskId": 4
          },
          {
            "id": 9,
            "title": "Refactor 3: Final cleanup for production readiness",
            "description": "Final refactor of the complete structured logging implementation for production readiness while ensuring all tests remain green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              8
            ],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Resilient Async HTTP Client",
        "description": "Develop an async HTTP client with exponential backoff for 429/503, supporting up to 10 concurrent requests.",
        "details": "Use httpx==0.27.0 with asyncio for concurrency. Integrate tenacity==8.2.2 for exponential backoff. Limit concurrency with asyncio.Semaphore(10). Centralize error handling for 429/503. Place client in /infrastructure/http_client.py.",
        "testStrategy": "Unit test retry logic and concurrency limits. Behave scenarios for simulated transient failures.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for basic async HTTP GET",
            "description": "Create a Behave scenario testing basic async HTTP GET request functionality and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Implement 1: Make basic GET scenario pass",
            "description": "Implement minimal async HTTP client with httpx to make the basic GET scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up basic HTTP client",
            "description": "Refactor basic HTTP client implementation for clarity and maintainability while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for concurrency control",
            "description": "Create a Behave scenario testing concurrent HTTP requests (max 10) and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              3
            ],
            "parentTaskId": 5
          },
          {
            "id": 5,
            "title": "Implement 2: Make concurrency scenario pass",
            "description": "Implement asyncio.Semaphore(10) concurrency control to make the concurrency scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              4
            ],
            "parentTaskId": 5
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up concurrency implementation",
            "description": "Refactor concurrency implementation for better structure while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              5
            ],
            "parentTaskId": 5
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for 429 rate limit handling",
            "description": "Create a Behave scenario testing HTTP 429 rate limit response handling and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              6
            ],
            "parentTaskId": 5
          },
          {
            "id": 8,
            "title": "Implement 3: Make 429 handling scenario pass",
            "description": "Implement basic exponential backoff for 429 responses using tenacity to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              7
            ],
            "parentTaskId": 5
          },
          {
            "id": 9,
            "title": "Refactor 3: Clean up backoff implementation",
            "description": "Refactor exponential backoff implementation for better maintainability while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              8
            ],
            "parentTaskId": 5
          },
          {
            "id": 10,
            "title": "Scenario 4: Write failing test for 503 service unavailable handling",
            "description": "Create a Behave scenario testing HTTP 503 service unavailable response handling and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              9
            ],
            "parentTaskId": 5
          },
          {
            "id": 11,
            "title": "Implement 4: Make 503 handling scenario pass",
            "description": "Extend exponential backoff to handle 503 responses to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              10
            ],
            "parentTaskId": 5
          },
          {
            "id": 12,
            "title": "Refactor 4: Final cleanup for production readiness",
            "description": "Final refactor of the complete async HTTP client for production readiness while ensuring all tests remain green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              11
            ],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Fetch Project Metadata from Azure DevOps REST APIs",
        "description": "Implement logic to fetch work-item types, fields, behaviors, teams, backlogs, and team settings for a given project.",
        "details": "Use azure-devops==7.1.0 or direct REST calls via httpx. Endpoints: /wit/workitemtypes, /wit/fields, /processes/{id}/behaviors, /core/teams, /work/backlogs, /work/teamsettings. Serialize results into domain objects. Place logic in /domain/metadata.py.",
        "testStrategy": "Behave scenarios for each endpoint. Unit test response parsing and error handling.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for basic project metadata fetch",
            "description": "Create a Behave scenario testing basic project metadata fetch and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Implement 1: Make basic metadata fetch scenario pass",
            "description": "Implement minimal project metadata fetch to make the basic scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up basic metadata fetch",
            "description": "Refactor basic metadata fetch implementation while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 6
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for work item types fetch",
            "description": "Create a Behave scenario testing work item types fetch and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              3
            ],
            "parentTaskId": 6
          },
          {
            "id": 5,
            "title": "Implement 2: Make work item types scenario pass",
            "description": "Implement work item types fetch to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              4
            ],
            "parentTaskId": 6
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up work item types implementation",
            "description": "Refactor work item types implementation while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              5
            ],
            "parentTaskId": 6
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for remaining metadata types",
            "description": "Create a Behave scenario testing fields, behaviors, teams, and backlogs fetch and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              6
            ],
            "parentTaskId": 6
          },
          {
            "id": 8,
            "title": "Implement 3: Make remaining metadata scenario pass",
            "description": "Implement fields, behaviors, teams, and backlogs fetch to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              7
            ],
            "parentTaskId": 6
          },
          {
            "id": 9,
            "title": "Refactor 3: Final cleanup for production readiness",
            "description": "Final refactor of complete project metadata fetch for production readiness while ensuring all tests remain green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              8
            ],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "Fetch Activity Metrics via OData Analytics v4",
        "description": "Query OData Analytics endpoints for monthly aggregates of work items, PRs, and pipeline runs.",
        "details": "Use httpx async client for OData queries. Endpoints: WorkItemsSnapshot, WorkItemRevisions, PullRequestEvents, /pipelines/{id}/runs. Aggregate by month using pandas==2.2.2 for in-memory grouping. Place logic in /domain/metrics.py.",
        "testStrategy": "Behave scenarios for metrics collection. Unit test aggregation logic and error handling.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for OData Analytics connection",
            "description": "Create a Behave scenario testing basic OData Analytics connection and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Implement 1: Make OData connection scenario pass",
            "description": "Implement minimal OData Analytics connection to make the basic scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up OData connection",
            "description": "Refactor OData connection implementation while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 7
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for work items aggregates",
            "description": "Create a Behave scenario testing work items monthly aggregates query and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              3
            ],
            "parentTaskId": 7
          },
          {
            "id": 5,
            "title": "Implement 2: Make work items aggregates scenario pass",
            "description": "Implement work items monthly aggregates query to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              4
            ],
            "parentTaskId": 7
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up work items aggregates",
            "description": "Refactor work items aggregates implementation while keeping tests green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              5
            ],
            "parentTaskId": 7
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for PRs and pipeline aggregates",
            "description": "Create a Behave scenario testing PRs and pipeline runs monthly aggregates and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [
              6
            ],
            "parentTaskId": 7
          },
          {
            "id": 8,
            "title": "Implement 3: Make PRs and pipeline aggregates scenario pass",
            "description": "Implement PRs and pipeline runs monthly aggregates query to make the scenario pass.",
            "details": "",
            "status": "pending",
            "dependencies": [
              7
            ],
            "parentTaskId": 7
          },
          {
            "id": 9,
            "title": "Refactor 3: Final cleanup for production readiness",
            "description": "Final refactor of complete OData Analytics implementation for production readiness while ensuring all tests remain green.",
            "details": "",
            "status": "pending",
            "dependencies": [
              8
            ],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Enrich Users via Microsoft Graph API",
        "description": "Lookup Azure DevOps identities in Microsoft Graph to fetch job title and mail, and annotate usage patterns.",
        "details": "Use msgraph-core==1.0.0 and httpx for Graph API calls. For each unique user ID, fetch /users/{id}. Annotate users with PR-heavy or work-item-heavy based on metrics. Place enrichment logic in /domain/user_enrichment.py.",
        "testStrategy": "Behave scenarios for user enrichment. Unit test Graph API integration and annotation logic.",
        "priority": "medium",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for Graph API user lookup",
            "description": "Create a Behave scenario testing basic Microsoft Graph API user lookup and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Implement 1: Make user lookup scenario pass and refactor",
            "description": "Implement minimal Graph API user lookup to make scenario pass, then refactor for maintainability.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "Scenario 2: Enrichment scenario, implement, and final refactor",
            "description": "Create scenario for job title and mail enrichment, implement to pass, then refactor for production readiness.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Serialize Domain Objects to JSON with orjson",
        "description": "Serialize all collected domain objects into a single portable JSON file using orjson.",
        "details": "Use orjson==3.9.10 for fast serialization. Ensure output matches published schema and is â‰¤50 MB for typical projects. Place serialization logic in /infrastructure/serialization.py.",
        "testStrategy": "Unit test serialization for schema compliance and file size. Behave scenario for end-to-end export.",
        "priority": "high",
        "dependencies": [
          6,
          7,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for JSON serialization",
            "description": "Create a Behave scenario testing basic JSON serialization with orjson and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 2,
            "title": "Implement 1: Make serialization scenario pass and refactor",
            "description": "Implement basic orjson serialization to make scenario pass, then refactor for maintainability.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 9
          },
          {
            "id": 3,
            "title": "Scenario 2: Complete serialization scenario, implement, and final refactor",
            "description": "Create scenario for complete domain objects serialization, implement to pass, then final refactor for production.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement Error Handling and Exit Codes",
        "description": "Handle errors per PRD: fatal exit for Analytics root unreachable, warnings for partial fetch failures, credential errors.",
        "details": "Centralize error handling in /cli/cli.py. Map scenarios to exit codes: 0 (success), 1 (partial), 2 (fatal Analytics error). Emit warnings array in JSON output. Log errors with context.",
        "testStrategy": "Behave scenarios for each error case. Unit test exit code logic and warning emission.",
        "priority": "high",
        "dependencies": [
          6,
          7,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for credential error handling",
            "description": "Create a Behave scenario testing credential error handling and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 2,
            "title": "Implement 1: Make credential error scenario pass and refactor",
            "description": "Implement credential error handling to make scenario pass, then refactor.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 10
          },
          {
            "id": 3,
            "title": "Scenario 2: Analytics and partial failure scenarios, implement, and final refactor",
            "description": "Create scenarios for Analytics root unreachable and partial fetch failures, implement, then final refactor.",
            "details": "",
            "status": "pending",
            "dependencies": [
              2
            ],
            "parentTaskId": 10
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement --skip-metrics Option",
        "description": "Support CLI option to export configuration only, skipping all Analytics queries.",
        "details": "Add --skip-metrics flag to CLI. Bypass metrics collection logic if set. Ensure output JSON omits metrics section when skipped.",
        "testStrategy": "Behave scenario for --skip-metrics. Unit test output structure.",
        "priority": "medium",
        "dependencies": [
          2,
          6,
          9,
          10
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for --skip-metrics option",
            "description": "Create a Behave scenario testing --skip-metrics option and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 2,
            "title": "Implement 1: Make skip-metrics scenario pass and final refactor",
            "description": "Implement --skip-metrics option to make scenario pass, then refactor for maintainability and production readiness.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 11
          }
        ]
      },
      {
        "id": 12,
        "title": "Publish and Version JSON Schema",
        "description": "Publish the JSON schema for the export artefact and version it for downstream consumers.",
        "details": "Define schema in /infrastructure/schema.json. Use jsonschema==4.22.0 for validation. Publish schema to repository and document versioning strategy.",
        "testStrategy": "Unit test schema validation. Behave scenario for schema compliance.",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for JSON schema publishing",
            "description": "Create a Behave scenario testing JSON schema publishing and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 2,
            "title": "Implement 1: Make schema publishing scenario pass and final refactor",
            "description": "Implement JSON schema publishing to make scenario pass, then refactor for versioning and production readiness.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 12
          }
        ]
      },
      {
        "id": 13,
        "title": "Acceptance Test Suite with Behave",
        "description": "Develop BDD acceptance tests mapping each requirement to Gherkin scenarios, executed against ephemeral Azure DevOps org.",
        "details": "Use behave==1.2.6 for BDD. Place features in /features. Automate ephemeral org setup in CI using azdo-demo-org. Map each PRD requirement to a scenario.",
        "testStrategy": "Run Behave suite in CI. Ensure all scenarios pass and failures are logged with rich context.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract PRD Requirements for Test Mapping",
            "description": "Review the Product Requirements Document (PRD) and extract all requirements that need to be mapped to acceptance tests.",
            "dependencies": [],
            "details": "Gather and document each PRD requirement to ensure comprehensive coverage in the test suite.",
            "status": "pending",
            "testStrategy": "Verify all PRD requirements are listed and none are omitted."
          },
          {
            "id": 2,
            "title": "Author Gherkin Scenarios for Each Requirement",
            "description": "Write Gherkin scenarios in .feature files for each PRD requirement, ensuring clarity and traceability.",
            "dependencies": [
              1
            ],
            "details": "Place all .feature files in the /features directory, with each scenario clearly mapped to its corresponding requirement.",
            "status": "pending",
            "testStrategy": "Review scenarios for completeness and direct mapping to requirements; peer review for Gherkin syntax."
          },
          {
            "id": 3,
            "title": "Implement Behave Step Definitions",
            "description": "Develop Python step definitions for all Gherkin scenarios using behave==1.2.6.",
            "dependencies": [
              2
            ],
            "details": "Ensure step definitions are modular, reusable, and placed in the appropriate /features/steps directory.",
            "status": "pending",
            "testStrategy": "Unit test step definitions for correctness; ensure all steps are covered and executable."
          },
          {
            "id": 4,
            "title": "Automate Ephemeral Azure DevOps Org Setup in CI",
            "description": "Integrate azdo-demo-org into CI to provision and tear down ephemeral Azure DevOps organizations for test execution.",
            "dependencies": [
              3
            ],
            "details": "Configure CI pipeline to automatically create and destroy orgs for each test run, ensuring isolation and repeatability.",
            "status": "pending",
            "testStrategy": "Validate org setup and teardown in CI logs; ensure no residual resources remain post-test."
          },
          {
            "id": 5,
            "title": "Execute and Validate Behave Suite in CI",
            "description": "Run the Behave acceptance test suite in CI against the ephemeral Azure DevOps org, capturing and reporting results.",
            "dependencies": [
              4
            ],
            "details": "Ensure all scenarios are executed, failures are logged with rich context, and results are published as CI artefacts.",
            "status": "pending",
            "testStrategy": "Confirm all scenarios pass; verify failure logs contain detailed context; ensure artefacts are accessible."
          }
        ]
      },
      {
        "id": 14,
        "title": "Continuous Integration Setup",
        "description": "Configure CI pipeline to run tests, Behave acceptance suite, and measure runtime and file size metrics.",
        "details": "Use GitHub Actions or Azure Pipelines. Steps: lint (ruff), unit tests (pytest), Behave acceptance, runtime/file size checks. Publish logs and artefacts. Place CI config in /.github/workflows/ci.yml.",
        "testStrategy": "Verify CI runs on PR and main. Ensure artefacts and logs are published. Behave failures block merge.",
        "priority": "medium",
        "dependencies": [
          13
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for CI pipeline",
            "description": "Create a Behave scenario testing CI pipeline setup and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 14
          },
          {
            "id": 2,
            "title": "Implement 1: Make CI pipeline scenario pass and final refactor",
            "description": "Implement CI pipeline configuration to make scenario pass, then refactor for metrics collection and production readiness.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 14
          }
        ]
      },
      {
        "id": 15,
        "title": "Documentation and CLI Help Verification",
        "description": "Document usage, options, error codes, and ensure CLI help matches PRD requirements.",
        "details": "Update README.md with usage, options, error handling, and schema info. Verify Click auto-generated help matches PRD. Document authentication precedence and troubleshooting.",
        "testStrategy": "Manual review of documentation. Behave scenario for --help output. Validate README completeness.",
        "priority": "medium",
        "dependencies": [
          2,
          11,
          12,
          14
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for CLI help documentation",
            "description": "Create a Behave scenario testing CLI help documentation and verify it fails initially.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 15
          },
          {
            "id": 2,
            "title": "Implement 1: Make CLI help scenario pass and final refactor",
            "description": "Implement CLI help documentation to make scenario pass, then refactor for PRD compliance and production readiness.",
            "details": "",
            "status": "pending",
            "dependencies": [
              1
            ],
            "parentTaskId": 15
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-07-18T20:14:51.299Z",
      "updated": "2025-07-19T00:59:17.786Z",
      "description": "Tasks for master context"
    }
  }
}