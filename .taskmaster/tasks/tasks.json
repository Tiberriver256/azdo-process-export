{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Repository and Screaming Architecture",
        "description": "Create the project repository with a Screaming Architecture folder structure, separating domain logic from infrastructure details.",
        "details": "Use Python 3.11+ for best async support. Structure folders as /domain, /infrastructure, /cli, /tests, /features. Add pyproject.toml for dependency management (prefer Poetry). Include .gitignore and README.md. Follow Screaming Architecture principles: domain objects and logic in /domain, API clients in /infrastructure, CLI entrypoint in /cli.",
        "testStrategy": "Verify folder structure matches Screaming Architecture. Ensure all modules are importable and initial test runner executes.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Project Repository and Initialize Git",
            "description": "Set up a new project repository and initialize it with Git version control.",
            "dependencies": [],
            "details": "Create a new directory for the project, initialize Git, and set up the initial commit. Add a .gitignore file tailored for Python projects.",
            "status": "done",
            "testStrategy": "Verify that the repository is initialized, .gitignore is present, and Git status shows a clean working tree."
          },
          {
            "id": 2,
            "title": "Establish Screaming Architecture Folder Structure",
            "description": "Create the required folder structure following Screaming Architecture principles.",
            "dependencies": [],
            "details": "Add /domain, /infrastructure, /cli, /tests, and /features folders. Ensure /domain is for domain logic, /infrastructure for API clients and utilities, /cli for CLI entrypoint, /tests for unit tests, and /features for Behave scenarios.",
            "status": "done",
            "testStrategy": "Check that all folders exist and match the prescribed structure. Confirm that each folder is empty or contains a placeholder __init__.py."
          },
          {
            "id": 3,
            "title": "Configure Dependency Management with Poetry",
            "description": "Set up pyproject.toml using Poetry for dependency management and Python version specification.",
            "dependencies": [],
            "details": "Initialize Poetry in the project root, specify Python 3.11+ in pyproject.toml, and add initial dependencies as needed. Ensure pyproject.toml is present and correctly configured.",
            "status": "done",
            "testStrategy": "Run 'poetry install' to verify dependencies are installed and Python version is enforced."
          },
          {
            "id": 4,
            "title": "Add Project Documentation and Metadata Files",
            "description": "Create README.md and ensure project metadata files are present.",
            "dependencies": [],
            "details": "Write a README.md describing the project, architecture, and setup instructions. Ensure .gitignore and pyproject.toml are included in the repository.",
            "status": "done",
            "testStrategy": "Verify README.md is informative and all metadata files are present in the repository root."
          },
          {
            "id": 5,
            "title": "Validate Initial Importability and Test Runner Setup",
            "description": "Ensure all modules are importable and set up an initial test runner.",
            "dependencies": [],
            "details": "Add __init__.py files as needed. Set up a basic test runner (e.g., pytest) in /tests. Confirm that all modules in /domain, /infrastructure, and /cli can be imported without errors.",
            "status": "done",
            "testStrategy": "Run the test runner to confirm it executes and imports all modules successfully."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement CLI Entry Point with Click",
        "description": "Develop the CLI interface using Click, supporting the 'process' verb and all required options, following TDD principles.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "All CLI development must be TDD-driven. For each CLI feature, begin by documenting the desired behavior in a Behave scenario in /tests/features/cli_basic.feature. Write a failing test in /tests/steps/cli_steps.py, then implement the CLI code in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py until the test passes. Refactor only after the test passes. Use Click v8.1+ for robust CLI parsing. Implement 'process' command with options: --project, --out, --pat, --log-level, --skip-metrics, --version, --help. Ensure auto-generated help text matches PRD. Document this workflow and ensure all code changes are test-driven.",
        "testStrategy": "For each CLI feature, write a Behave scenario describing expected CLI invocation and output in /tests/features/cli_basic.feature. Implement failing step definitions in /tests/steps/cli_steps.py. Develop CLI code in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py until tests pass. Unit test CLI parsing and option handling. Refactor only after passing tests.",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up CLI Project Structure for TDD",
            "description": "Create the initial CLI project structure and ensure TDD workflow is established.",
            "status": "done",
            "dependencies": [],
            "details": "Create /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py. Set up /tests/features/cli_basic.feature and /tests/steps/cli_steps.py for Behave-driven development. Ensure CLI entry point is ready for TDD-based implementation.",
            "testStrategy": "Verify that the CLI entry point runs without errors and displays a basic help message. Confirm that Behave scenarios can be executed and fail as expected before implementation."
          },
          {
            "id": 2,
            "title": "Implement 'process' Command with Required Options via TDD",
            "description": "Develop the 'process' command in Click using TDD: start with Behave scenario, failing test, then CLI code.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Document the expected behavior for the 'process' command and its options (--project, --out, --pat, --log-level, --skip-metrics) in /tests/features/cli_basic.feature. Write failing step definitions in /tests/steps/cli_steps.py. Implement the command in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py until tests pass.",
            "testStrategy": "Unit test option parsing and validation for all supported flags. Behave scenario must pass for CLI invocation and option handling."
          },
          {
            "id": 3,
            "title": "Add Global Options and Version/Help Support via TDD",
            "description": "Integrate --version and --help options using TDD: scenario, failing test, then implementation.",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "Document expected help and version output in /tests/features/cli_basic.feature. Write failing step definitions in /tests/steps/cli_steps.py. Implement global options in CLI code until tests pass. Ensure help/version output matches PRD.",
            "testStrategy": "Compare CLI help and version output against PRD; Behave scenarios must pass for help/version invocation."
          },
          {
            "id": 4,
            "title": "Validate Option Handling and Error Messaging via TDD",
            "description": "Implement robust error handling for missing or invalid options using TDD: scenario, failing test, then implementation.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Document error cases and expected messages in /tests/features/cli_basic.feature. Write failing step definitions in /tests/steps/cli_steps.py. Implement error handling in CLI code until tests pass.",
            "testStrategy": "Unit test error cases and verify error messages match PRD expectations. Behave scenarios must pass for error handling."
          },
          {
            "id": 5,
            "title": "Integrate CLI with Behave and Unit Tests via TDD",
            "description": "Write Behave scenarios and unit tests to validate CLI invocation, option handling, and output structure, following TDD.",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "Ensure comprehensive test coverage for CLI behaviors, including all supported options and error cases. All new features must begin with a Behave scenario and failing test before implementation.",
            "testStrategy": "Implement Behave scenarios for CLI usage; write unit tests for option parsing and output. All tests must pass before refactoring."
          },
          {
            "id": 6,
            "title": "Document TDD Workflow for CLI Development",
            "description": "Create documentation outlining the TDD workflow for CLI development, including feature file creation, failing test writing, and implementation steps.",
            "status": "done",
            "dependencies": [],
            "details": "Write a README or developer guide describing the TDD process for CLI features: start with a Behave scenario in /tests/features/cli_basic.feature, write failing step definitions in /tests/steps/cli_steps.py, implement CLI code in /azdo_process_export/cli/main.py and /azdo_process_export/cli/__init__.py, and refactor only after tests pass.",
            "testStrategy": "Review documentation for clarity and completeness. Ensure it covers all required TDD steps and references correct file locations."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Authentication Logic",
        "description": "Support DefaultAzureCredential chain and PAT override for Azure DevOps and OData APIs, following latest best practices for credential precedence, error handling, and logging.",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "high",
        "details": "Implement authentication logic in /infrastructure/auth.py. If --pat is provided, always use it for Basic Auth and do not fallback to Azure AD if PAT fails. If no PAT is provided, use DefaultAzureCredential from azure-identity==1.14.0 for Bearer token authentication. Ensure correct token scope for both Azure DevOps and OData endpoints. Centralize error handling and structured JSON logging for all credential operations and failures. Emit clear, actionable error messages and map fatal credential errors to exit code 2. Document credential precedence and troubleshooting steps in README and CLI help.",
        "testStrategy": "Use Behave scenarios to test both credential paths (PAT and Azure AD), including mocked failures for each. Verify error messaging, exit codes, and structured JSON logging output. Ensure correct token scopes are used for Azure DevOps and OData endpoints. Validate documentation updates for credential precedence and troubleshooting.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for basic PAT authentication",
            "description": "Create a Behave scenario testing basic PAT authentication and verify it fails initially.",
            "details": "<info added on 2025-07-18T21:28:17.526Z>\nBehave scenario for basic PAT authentication is present in /tests/features/authentication.feature. Missing step definitions for structured log and authentication header checks have been added to /tests/steps/cli_steps.py; these steps currently fail as expected, confirming the scenario is initially red. Next, run Behave to verify the test fails, then begin implementing the authentication logic to make the scenario pass.\n</info added on 2025-07-18T21:28:17.526Z>\n<info added on 2025-07-18T21:28:40.731Z>\nUpdate Behave configuration to recognize the custom steps directory by setting steps_dir in behave.ini or command-line options, or move/copy the step definitions from /tests/steps/cli_steps.py to /features/steps/cli_steps.py so Behave can discover them. After resolving the steps directory issue, rerun Behave to confirm the scenario fails as expected, ensuring the test setup is correct before proceeding to implement authentication logic.\n</info added on 2025-07-18T21:28:40.731Z>\n<info added on 2025-07-18T21:29:22.134Z>\nImplement the missing step definitions for the PAT authentication scenario, including 'Given I have a valid Personal Access Token', in /features/steps/cli_steps.py. Once all steps are defined and the scenario is recognized by Behave, proceed to implement minimal PAT authentication logic in /infrastructure/auth.py to make the test pass. This should include accepting a PAT via CLI, using it for Basic Auth, and ensuring the authentication header is set correctly. Run Behave to confirm the scenario passes, verifying both the step definitions and the initial authentication implementation.\n</info added on 2025-07-18T21:29:22.134Z>\n<info added on 2025-07-18T21:31:11.268Z>\nMigrate environment.py from /tests to /features to align with Behave's standard directory structure. Move all .feature files to the root of /features and consolidate step definitions in /features/steps. Ensure all BDD test files, including environment.py and step definitions, are located within the /features directory for proper Behave discovery and execution. Update any relevant Behave configuration or CI scripts to reference the new /features structure.\n</info added on 2025-07-18T21:31:11.268Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 2,
            "title": "Implement 1: Make basic PAT authentication scenario pass",
            "description": "Implement minimal PAT authentication in /infrastructure/auth.py to make the basic PAT scenario pass.",
            "details": "<info added on 2025-07-18T22:04:15.523Z>\nMinimal PAT authentication has been implemented in the CLI: the process command now invokes get_auth_headers(pat), logs authentication success or failure, and exits with code 0 on success or 2 on failure. The Behave scenario for PAT authentication is integrated with the CLI logic. Behave was executed, but all scenarios were skipped, likely due to missing or misconfigured tags or step definitions. No code errors were found in the CLI or authentication modules. Next step is to debug Behave configuration to ensure scenarios run and validate that the test passes.\n</info added on 2025-07-18T22:04:15.523Z>\n<info added on 2025-07-18T22:05:27.318Z>\nUpdate the CLI to emit a structured JSON log message when authentication succeeds using a PAT credential. The log event should include the correct event name (e.g., \"authentication_success\"), the credential source (\"PAT\"), and any other fields required by the Behave test expectation. Ensure the log format matches the test's requirements so the scenario step \"And the structured log should contain authentication success with PAT credential source\" passes.\n</info added on 2025-07-18T22:05:27.318Z>",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 3
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up PAT authentication implementation",
            "description": "Refactor PAT authentication implementation for clarity and maintainability while keeping tests green.",
            "details": "",
            "status": "done",
            "dependencies": [
              2
            ],
            "parentTaskId": 3
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for DefaultAzureCredential fallback",
            "description": "Create a Behave scenario testing DefaultAzureCredential fallback when no PAT is provided and verify it fails initially.",
            "details": "",
            "status": "done",
            "dependencies": [
              3
            ],
            "parentTaskId": 3
          },
          {
            "id": 5,
            "title": "Implement 2: Make DefaultAzureCredential scenario pass",
            "description": "Implement DefaultAzureCredential fallback with correct Azure DevOps token scope to make the scenario pass.",
            "details": "",
            "status": "done",
            "dependencies": [
              4
            ],
            "parentTaskId": 3
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up DefaultAzureCredential implementation",
            "description": "Refactor DefaultAzureCredential implementation for better structure while keeping tests green.",
            "details": "",
            "status": "done",
            "dependencies": [
              5
            ],
            "parentTaskId": 3
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for error handling and logging",
            "description": "Create a Behave scenario testing authentication error handling and structured JSON logging and verify it fails initially.",
            "details": "",
            "status": "done",
            "dependencies": [
              6
            ],
            "parentTaskId": 3
          },
          {
            "id": 8,
            "title": "Implement 3: Make error handling and logging scenario pass",
            "description": "Implement structured JSON logging and centralized error handling with clear error messages to make the scenario pass.",
            "details": "",
            "status": "done",
            "dependencies": [
              7
            ],
            "parentTaskId": 3
          },
          {
            "id": 9,
            "title": "Scenario 4: Write failing test for documentation and help",
            "description": "Create a Behave scenario testing credential precedence documentation and troubleshooting and verify it fails initially.",
            "details": "",
            "status": "done",
            "dependencies": [
              8
            ],
            "parentTaskId": 3
          },
          {
            "id": 10,
            "title": "Implement 4: Make documentation scenario pass and final refactor",
            "description": "Implement documentation updates for README and CLI help to make the scenario pass, then final refactor for production readiness.",
            "details": "",
            "status": "done",
            "dependencies": [
              9
            ],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Structured JSON Logging",
        "description": "Add structured logging per Better Stack guidance, supporting info/debug/trace levels and JSON output.",
        "details": "Use structlog==23.2.0 for JSON logging. No root logger; configure per module. Support --log-level option. Include timestamps, levels, and trace context. Output logs to stdout and optionally to file. Place logging config in /infrastructure/logging.py.",
        "testStrategy": "Unit test log output format. Behave scenarios for log-level switching. Validate logs are JSON and contain required fields.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for basic structured logging setup",
            "description": "Create a Behave scenario that tests basic structured JSON logger initialization and verify it fails initially.",
            "details": "<info added on 2025-07-20T21:00:48.922Z>\nThe basic structured logging setup has been fully implemented and verified. All requirements for structured JSON logging using structlog==23.2.0 are met, including logger initialization, required JSON fields, log level and file output options, and Better Stack compliance. All Behave scenarios in features/logging.feature are passing, confirming correct functionality and test coverage for info, debug, and trace levels, file output, trace context, and sensitive data handling. No further action is required for this subtask.\n</info added on 2025-07-20T21:00:48.922Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Implement 1: Make basic logging scenario pass",
            "description": "Implement minimal code to make the basic structured logging scenario pass - create logger configuration and basic JSON output.",
            "details": "",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up basic logging implementation",
            "description": "Clean up the basic logging implementation for clarity and maintainability while ensuring tests remain green.",
            "details": "",
            "status": "done",
            "dependencies": [
              2
            ],
            "parentTaskId": 4
          },
          {
            "id": 4,
            "title": "Scenario 2: Write failing test for log level configuration",
            "description": "Create a Behave scenario testing different log levels (info, debug, trace) and verify it fails initially.",
            "details": "",
            "status": "done",
            "dependencies": [
              3
            ],
            "parentTaskId": 4
          },
          {
            "id": 5,
            "title": "Implement 2: Make log level scenario pass",
            "description": "Implement log level configuration support to make the log level scenario pass.",
            "details": "",
            "status": "done",
            "dependencies": [
              4
            ],
            "parentTaskId": 4
          },
          {
            "id": 6,
            "title": "Refactor 2: Clean up log level implementation",
            "description": "Refactor log level implementation for better structure and maintainability while keeping tests green.",
            "details": "",
            "status": "done",
            "dependencies": [
              5
            ],
            "parentTaskId": 4
          },
          {
            "id": 7,
            "title": "Scenario 3: Write failing test for Better Stack compliance",
            "description": "Create a Behave scenario testing Better Stack guidance compliance for structured logging and verify it fails initially.",
            "details": "",
            "status": "done",
            "dependencies": [
              6
            ],
            "parentTaskId": 4
          },
          {
            "id": 8,
            "title": "Implement 3: Make Better Stack compliance scenario pass",
            "description": "Implement Better Stack compliance features to make the compliance scenario pass.",
            "details": "",
            "status": "done",
            "dependencies": [
              7
            ],
            "parentTaskId": 4
          },
          {
            "id": 9,
            "title": "Refactor 3: Final cleanup for production readiness",
            "description": "Final refactor of the complete structured logging implementation for production readiness while ensuring all tests remain green.",
            "details": "",
            "status": "done",
            "dependencies": [
              8
            ],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Resilient Async HTTP Client",
        "description": "Develop an async Azure DevOps API client using httpx and asyncio, evolving resilience and concurrency features only as needed based on actual failures and requirements.",
        "status": "done",
        "dependencies": [
          3,
          4
        ],
        "priority": "high",
        "details": "Begin with the simplest possible async client for a real Azure DevOps API endpoint (e.g., listing projects or work items) using httpx==0.27.0 and asyncio. Do not add concurrency, retry, or error handling until tests or real usage demonstrate the need. Avoid premature optimizations such as hardcoded concurrency limits, complex backoff, or external retry libraries. Place the client in /infrastructure/http_client.py. Add complexity only in response to failing tests or encountered issues, following strict TDD and YAGNI principles.",
        "testStrategy": "Start with a failing Behave scenario for a real Azure DevOps API call. Add further scenarios only as actual needs arise (e.g., retries on 429, concurrency for multiple operations). Use evidence-driven development: only add tests and implementation for features actually required or failing.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for minimal Azure DevOps API call",
            "description": "Create a Behave scenario for a minimal async call to a real Azure DevOps API endpoint (e.g., list projects) and verify it fails initially.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Implement 1: Make minimal Azure DevOps API scenario pass",
            "description": "Implement the simplest possible async Azure DevOps API client using httpx to make the minimal scenario pass. No retries, concurrency, or special error handling yet.",
            "details": "",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Refactor 1: Clean up minimal client implementation",
            "description": "Refactor the minimal Azure DevOps API client for clarity and maintainability while keeping tests green.",
            "details": "",
            "status": "done",
            "dependencies": [
              2
            ],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Fetch Basic Project Metadata from Azure DevOps REST APIs",
        "description": "Make the user-facing BDD scenario pass: 'As a delivery lead, I want to export basic project information from Azure DevOps so that I can understand the project structure and configuration'",
        "status": "in-progress",
        "dependencies": [
          5
        ],
        "priority": "high",
        "details": "Implement end-to-end functionality to make this scenario pass: Given I have access to an Azure DevOps project 'Demo Project' and valid authentication credentials, When I run 'azdo-process-export process 'Demo Project' --out project.json', Then the command should complete successfully and the output file should contain project name, description, and team information with structured logs showing project metadata retrieval.",
        "testStrategy": "Single BDD scenario testing the complete user workflow from CLI command to JSON output file generation with project metadata.",
        "subtasks": [
          {
            "id": 4,
            "title": "RED: Write failing end-to-end CLI scenario for project metadata export",
            "description": "Create the main user-facing BDD scenario that tests the complete CLI workflow from command execution to JSON file output.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "Create a new BDD scenario in features/cli_integration.feature that captures the user story: 'Given I have access to an Azure DevOps project Demo Project and valid authentication credentials, When I run azdo-process-export process Demo Project --out project.json, Then the command should complete successfully and the output file should contain project name, description, and team information with structured logs showing project metadata retrieval.' This scenario should fail because the CLI integration doesn't exist yet. Focus only on the test, no implementation code.",
            "testStrategy": "Single failing BDD scenario that represents the complete user workflow from CLI command to JSON output file generation"
          },
          {
            "id": 5,
            "title": "GREEN: Minimal CLI integration to make end-to-end scenario pass",
            "description": "Do the absolute minimum implementation to make the failing CLI scenario pass end-to-end.",
            "status": "pending",
            "dependencies": [
              4
            ],
            "details": "Update cli/cli.py to integrate ProjectMetadataService with the 'process' command. Add the minimal code needed to: 1) Accept 'Demo Project' as project argument, 2) Accept --out parameter for JSON output file, 3) Call ProjectMetadataService to fetch project metadata, 4) Write basic JSON output to specified file, 5) Add basic structured logging. Focus on making it work, not making it perfect. Build upon existing authentication and logging infrastructure from completed tasks.",
            "testStrategy": "Verify the end-to-end BDD scenario now passes with minimal implementation"
          },
          {
            "id": 6,
            "title": "REFACTOR: Clean up and optimize CLI integration implementation",
            "description": "Refactor the minimal implementation for maintainability, performance, and production readiness.",
            "status": "pending",
            "dependencies": [
              5
            ],
            "details": "Clean up the CLI integration code for production quality: 1) Improve error handling and edge cases, 2) Enhance structured logging with proper context and trace information, 3) Optimize JSON output format and file writing, 4) Add input validation and user-friendly error messages, 5) Ensure proper resource cleanup and exception handling, 6) Add comprehensive documentation and type hints. Ensure all existing tests still pass after refactoring.",
            "testStrategy": "All BDD scenarios continue to pass after refactoring, with improved error handling and logging quality"
          },
          {
            "id": 1,
            "title": "Create Project Domain Models",
            "description": "Define core domain models for Azure DevOps projects including Project, Collection, and Team dataclasses with proper type hints and validation.",
            "dependencies": [],
            "details": "Create domain/models.py with dataclasses for Project (id, name, description, url, state, revision, visibility), Collection (id, name, url), and Team (id, name, description, url, project_id). Use typing annotations and optional validation with pydantic or dataclasses validators. Include __str__ and __repr__ methods for debugging.\n<info added on 2025-07-20T22:38:24.184Z>\nSuccessfully implemented the domain models with Collection dataclass added (id, name, url, collection_url fields), Project dataclass enhanced with collection and default_team optional fields, and verified all existing Team model fields are present. The models now support the complete Azure DevOps project metadata structure with proper typing and follow existing code patterns.\n</info added on 2025-07-20T22:38:24.184Z>",
            "status": "done",
            "testStrategy": "Unit tests for model instantiation, field validation, and serialization/deserialization"
          },
          {
            "id": 2,
            "title": "Add Basic Project Metadata Feature",
            "description": "Create Behave feature file and step definitions for project metadata fetching scenarios using BDD approach.",
            "dependencies": [
              "6.1"
            ],
            "details": "Create features/project_metadata.feature with scenario 'Fetch project details by ID'. Define step definitions in features/steps/project_metadata_steps.py. Include Given (authenticated client), When (fetch project by ID), Then (verify project details). Start with failing test following RED phase of TDD.\n<info added on 2025-07-20T22:43:00.347Z>\nBDD Feature Implementation Complete: Successfully implemented the complete Behave feature and step definitions for project metadata with files created (features/project_metadata.feature, features/steps/project_metadata_steps.py, azdo_process_export/domain/metadata.py), all 3 scenarios passing with 25 steps implemented, and RED phase complete for TDD cycle.\n</info added on 2025-07-20T22:43:00.347Z>\n<info added on 2025-07-21T00:07:48.333Z>\nBDD test verification completed successfully. All 3 scenarios are properly failing in RED phase as expected: \"Fetch project details by ID\" fails at metadata retrieval, \"Handle project not found\" fails with NotImplementedError instead of ProjectNotFoundError, and \"List all projects in organization\" fails at list projects call. Tests are using real ProjectMetadataService instantiation without mocks, step definitions correctly call service methods, and proper error handling is in place. RED phase confirmed and ready to proceed to GREEN phase implementation in next subtask.\n</info added on 2025-07-21T00:07:48.333Z>",
            "status": "done",
            "testStrategy": "Single Behave scenario covering happy path for project metadata retrieval"
          },
          {
            "id": 3,
            "title": "Implement ProjectMetadataService",
            "description": "Create the core service class in domain/metadata.py with Azure DevOps integration for fetching project information.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "Implement ProjectMetadataService class with methods: get_project_by_id(), list_projects(). Use azure-devops==7.1.0 CoreClient for /core/projects/{projectId} endpoint. Handle authentication via Personal Access Token. Convert API responses to domain model objects. Follow dependency injection pattern for testability.\n<info added on 2025-07-21T00:19:01.883Z>\nImplementation successfully completed with real Azure DevOps integration using azure-devops library. Added BasicAuthentication with PAT support, CoreClient integration, and proper error handling mapping (404→ProjectNotFoundError, 401→AuthenticationError, 5xx→ServiceUnavailableError). Both get_project_by_id() and list_projects() methods implemented with domain model conversion. Tests now fail with ServiceUnavailableError instead of NotImplementedError, confirming real API integration is working with expected failures due to demo credentials. Ready for real environment testing.\n</info added on 2025-07-21T00:19:01.883Z>\n<info added on 2025-07-21T00:37:10.143Z>\nFinal implementation and testing complete with comprehensive BDD test coverage. Environment variable integration successfully implemented with context.test_organization, context.test_pat, and context.test_project_id configuration. Feature file updated to use business-friendly language (\"Given a test project exists\" vs hardcoded IDs). All 25 BDD steps now pass with real Azure DevOps API integration - no mocks used per project philosophy. Final test results: 1 feature passed, 3 scenarios passed, confirming production-ready implementation with proper error handling and environment-driven configuration.\n</info added on 2025-07-21T00:37:10.143Z>",
            "status": "done",
            "testStrategy": "Unit tests with mocked Azure DevOps client, integration tests with real API calls"
          }
        ]
      },
      {
        "id": 7,
        "title": "Fetch Activity Metrics via OData Analytics v4",
        "description": "Make the user-facing BDD scenario pass: 'As a process coach, I want to export activity metrics from Azure DevOps so that I can analyze team productivity trends and identify bottlenecks'",
        "status": "pending",
        "dependencies": [
          5
        ],
        "priority": "high",
        "details": "Implement end-to-end functionality to make this scenario pass: Given I have access to an Azure DevOps project with historical data and valid Analytics permissions, When I run 'azdo-process-export process 'Analytics Project' --out metrics.json', Then the command should complete successfully, the output file should contain monthly work item, PR, and pipeline run metrics, and the structured logs should show Analytics data retrieval. Use httpx async client for OData queries. Endpoints: WorkItemsSnapshot, WorkItemRevisions, PullRequestEvents, /pipelines/{id}/runs. Aggregate by month using pandas==2.2.2 for in-memory grouping. Place logic in /domain/metrics.py.",
        "testStrategy": "Single BDD scenario testing the complete user workflow from CLI command to JSON output file generation with activity metrics.",
        "subtasks": [
          {
            "id": 1,
            "title": "RED: Write failing BDD scenario for activity metrics export",
            "description": "Create the Behave scenario for the user story ensuring it fails because OData Analytics functionality doesn't exist yet.",
            "status": "pending",
            "dependencies": [],
            "details": "Create features/activity_metrics.feature with the scenario 'Export activity metrics successfully'. Define step definitions in features/steps/activity_metrics_steps.py. Include Given (Azure DevOps project with historical data and Analytics permissions), When (run CLI command 'azdo-process-export process Analytics Project --out metrics.json'), Then (verify successful completion, output file contents with monthly metrics, and structured logs showing Analytics data retrieval). Ensure the test fails because no OData Analytics implementation exists. No implementation code - just the failing test following strict TDD RED phase.",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "GREEN: Minimal OData Analytics implementation to make scenario pass",
            "description": "Do the absolute minimum implementation to make the BDD scenario pass, focusing on making it work rather than perfect.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Implement minimal ActivityMetricsService class in domain/metrics.py with basic OData queries for work items, PRs, and pipeline metrics. Use httpx async client for simple queries to WorkItemsSnapshot, WorkItemRevisions, PullRequestEvents, and /pipelines/{id}/runs endpoints. Create basic monthly aggregation using pandas==2.2.2. Connect to CLI 'process' command to generate metrics.json output. Build upon existing authentication infrastructure from completed Task 3. Focus solely on making the BDD scenario pass - no error handling, pagination, or optimization yet.",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "REFACTOR: Clean up and optimize the Analytics implementation",
            "description": "Refactor the minimal implementation for maintainability, performance, and production readiness while ensuring all tests still pass.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Refactor the working Analytics implementation for production quality. Add proper error handling for API failures and network timeouts. Implement pagination for large datasets. Optimize aggregation logic and memory usage. Add comprehensive structured logging for Analytics data retrieval progress. Improve code organization and separation of concerns. Add input validation and edge case handling. Ensure the BDD scenario and any additional tests continue to pass after refactoring. Focus on maintainability, performance, and robustness.",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Enrich Users via Microsoft Graph API",
        "description": "Lookup Azure DevOps identities in Microsoft Graph to fetch job title and mail, and annotate usage patterns to enable correlation of activity with team member roles.",
        "status": "pending",
        "dependencies": [
          6,
          7
        ],
        "priority": "medium",
        "details": "Implement Microsoft Graph integration to make the user-facing BDD scenario pass. Use msgraph-core==1.0.0 and httpx for Graph API calls. For each unique user ID, fetch /users/{id} to get job title and email. Annotate users with activity patterns (PR-heavy or work-item-heavy) based on metrics. Place enrichment logic in /domain/user_enrichment.py. Focus on enabling data analysts to correlate activity patterns with team member roles and responsibilities.",
        "testStrategy": "Single BDD scenario: 'Export enriched user information successfully' - validates that azdo-process-export command produces output with enriched user data including job titles, email addresses, and activity pattern annotations.",
        "subtasks": [
          {
            "id": 1,
            "title": "RED: Write failing BDD scenario for enriched user export",
            "description": "Create the Behave scenario that defines the expected behavior for user enrichment functionality.",
            "status": "pending",
            "dependencies": [],
            "details": "Create the BDD scenario 'Export enriched user information successfully' that tests the complete user workflow. The scenario should fail initially because Graph API functionality doesn't exist yet. Focus only on writing the test specification, not implementation code.",
            "testStrategy": "Verify the scenario fails with clear error indicating missing Graph API integration"
          },
          {
            "id": 2,
            "title": "GREEN: Minimal Graph API implementation to make scenario pass",
            "description": "Implement the absolute minimum Graph API functionality required to make the BDD scenario pass.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create basic Microsoft Graph API integration in /domain/user_enrichment.py using msgraph-core==1.0.0 and httpx. Implement minimal user lookup by ID to fetch job title and email. Focus on making the BDD scenario pass with the simplest possible implementation.",
            "testStrategy": "Verify the BDD scenario now passes with basic enriched user data in output"
          },
          {
            "id": 3,
            "title": "REFACTOR: Clean up and optimize the user enrichment implementation",
            "description": "Refactor the user enrichment implementation for maintainability, performance, and completeness.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Add proper error handling, caching, and activity pattern annotation logic (PR-heavy vs work-item-heavy). Optimize Graph API calls, improve code structure, and ensure robust integration with the export command workflow. Maintain all existing test coverage.",
            "testStrategy": "Verify all tests still pass after refactoring and enriched data quality is improved"
          }
        ]
      },
      {
        "id": 9,
        "title": "Serialize Domain Objects to JSON with orjson",
        "description": "Implement JSON serialization with orjson to enable generation of a complete portable JSON export containing all Azure DevOps project data.",
        "status": "pending",
        "dependencies": [
          6,
          7,
          8
        ],
        "priority": "high",
        "details": "Make the user-facing BDD scenario pass: 'Generate complete portable JSON export'. Use orjson==3.9.10 for fast serialization. Ensure output matches published schema and is ≤50 MB for typical projects. Place serialization logic in /infrastructure/serialization.py. The JSON should contain project metadata, activity metrics, and user information in a portable, self-contained format.",
        "testStrategy": "Implement the BDD scenario: 'Given I have access to an Azure DevOps project with full data, When I run \"azdo-process-export process 'Complete Project' --out complete.json\", Then the command should complete successfully, And the output file should be a valid JSON under 50MB, And the JSON should contain project metadata, activity metrics, and user information, And the file should be portable and self-contained.'",
        "subtasks": [
          {
            "id": 1,
            "title": "Write failing BDD scenario for complete JSON export (RED)",
            "description": "Create the Behave scenario for the user story that demonstrates generating a complete portable JSON export. This test should fail because JSON serialization doesn't exist yet.",
            "status": "pending",
            "dependencies": [],
            "details": "Write failing BDD scenario: 'Generate complete portable JSON export' with steps covering command execution, file validation, size constraints, and content verification. Ensure it fails because JSON serialization is not implemented. No implementation code, just the failing test.",
            "testStrategy": "Verify the scenario fails with appropriate error indicating missing JSON serialization functionality."
          },
          {
            "id": 2,
            "title": "Minimal orjson implementation to make scenario pass (GREEN)",
            "description": "Implement the absolute minimum JSON serialization functionality using orjson to make the BDD scenario pass, focusing only on making it work.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Do the minimum to make the BDD scenario pass. Basic JSON serialization of domain objects using orjson==3.9.10. Implement in /infrastructure/serialization.py. Focus on making it work, not perfect - just get the test passing.",
            "testStrategy": "Verify the BDD scenario now passes with basic JSON output containing required data elements."
          },
          {
            "id": 3,
            "title": "Clean up and optimize JSON serialization (REFACTOR)",
            "description": "Refactor the JSON serialization implementation for maintainability, performance, and compliance while ensuring all tests still pass.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Refactor for maintainability and performance. Add schema compliance, size optimization (≤50 MB), proper error handling, and ensure portable, self-contained format. All tests must still pass after refactoring.",
            "testStrategy": "Verify all existing tests pass and output meets quality standards: valid JSON, under 50MB, contains all required sections, and is properly structured."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement Error Handling and Exit Codes",
        "description": "As a user, I want clear error messages and appropriate exit codes when something goes wrong so that I can troubleshoot issues and understand the tool's status.",
        "status": "pending",
        "dependencies": [
          6,
          7,
          9
        ],
        "priority": "high",
        "details": "Implement proper error handling and exit codes to make BDD scenarios pass. Handle authentication failures with exit code 2 and clear error messages. Handle partial data failures with exit code 1, warnings, and structured output. Centralize error handling in /cli/cli.py with structured logging context.",
        "testStrategy": "Two primary BDD scenarios: 1) Authentication failure handling with invalid credentials, 2) Partial data failure handling with limited permissions. Verify exit codes, error messages, and structured JSON output with warnings array.",
        "subtasks": [
          {
            "id": 1,
            "title": "RED: Write failing BDD scenario for CLI output file",
            "description": "Create the Behave scenario that validates CLI file output behavior and ensure it fails because file output handling doesn't exist.",
            "status": "pending",
            "dependencies": [],
            "details": "Create a BDD scenario that tests the CLI's --out parameter functionality for writing export results to a file. The scenario should validate that the CLI can write structured JSON output with error handling information to a specified file path. Focus on the CLI user experience rather than implementation details. This test should fail initially because file output handling is not yet implemented.",
            "testStrategy": "Write a failing Behave scenario that expects file output functionality to work but will fail due to missing implementation."
          },
          {
            "id": 2,
            "title": "GREEN: Minimal file output handling to make scenario pass",
            "description": "Implement the absolute minimum file output handling with --out parameter to make the BDD scenario pass.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Add basic file writing functionality to the CLI with --out parameter support. Implement only what's necessary to make the BDD scenario pass - basic file writing without robust error handling or path validation. Focus on making the test pass rather than creating a perfect implementation.",
            "testStrategy": "Ensure the BDD scenario from subtask 1 passes with minimal implementation."
          },
          {
            "id": 3,
            "title": "REFACTOR: Clean up file output implementation",
            "description": "Refactor the file output implementation for robustness and proper error handling while ensuring all tests still pass.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Enhance the basic file output implementation with proper error handling, path validation, permissions checking, and cleanup mechanisms. Add comprehensive error handling for file system operations, validate output paths, handle permission errors gracefully, and ensure proper resource cleanup. Maintain backward compatibility and ensure all existing tests continue to pass.",
            "testStrategy": "Verify that all BDD scenarios continue to pass after refactoring, and add additional edge case testing for robust file handling."
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement --skip-metrics Option",
        "description": "Support CLI option to export configuration only, skipping all Analytics queries.",
        "status": "pending",
        "dependencies": [
          2,
          6,
          9,
          10
        ],
        "priority": "medium",
        "details": "As a user, I want to export only configuration data without metrics when I need fast results or don't have Analytics permissions. Implement --skip-metrics flag to CLI that bypasses metrics collection logic and ensures output JSON omits metrics section when skipped.",
        "testStrategy": "Single BDD scenario: Export configuration only with --skip-metrics. Verify command completes successfully in under 30 seconds, output contains project configuration without metrics, and structured logs show metrics collection was skipped.",
        "subtasks": [
          {
            "id": 1,
            "title": "Write failing BDD scenario for complete CLI workflow",
            "description": "Create the Behave scenario for the complete end-to-end workflow with --skip-metrics flag.",
            "status": "pending",
            "dependencies": [],
            "details": "Create BDD scenario: Given I have access to an Azure DevOps project and may not have Analytics permissions, when I run 'azdo-process-export process Config Project --skip-metrics --out config.json', then the command should complete successfully in under 30 seconds, output file should contain project configuration without metrics, and structured logs should show metrics collection was skipped. Ensure it fails because full integration doesn't exist yet.",
            "testStrategy": "Focus on the complete user journey from command execution to JSON file output validation."
          },
          {
            "id": 2,
            "title": "Minimal integration to make end-to-end scenario pass",
            "description": "Do the absolute minimum to connect all components and make the complete workflow work.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Implement basic CLI flag parsing for --skip-metrics, integrate existing authentication and configuration services, bypass metrics collection when flag is set, ensure JSON output omits metrics section, and add structured logging to indicate metrics were skipped. Focus on making the complete workflow functional.",
            "testStrategy": "Verify the BDD scenario passes with minimal viable implementation."
          },
          {
            "id": 3,
            "title": "Optimize and improve the integration",
            "description": "Refactor for performance, maintainability, and robustness while ensuring all tests still pass.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Add proper error handling for edge cases, optimize performance for large configurations, improve code structure and maintainability, enhance monitoring and observability, and ensure robust handling of various project configurations. All existing tests must continue to pass.",
            "testStrategy": "Run full test suite to ensure no regressions and validate performance improvements."
          }
        ]
      },
      {
        "id": 12,
        "title": "Publish and Version JSON Schema",
        "description": "As a downstream tool developer, I want access to a versioned JSON schema for the export file so that I can build automated processing tools that validate the data structure and adapt to schema changes.",
        "status": "pending",
        "dependencies": [
          9
        ],
        "priority": "medium",
        "details": "Implement BDD scenario: Given I have successfully exported project data to a JSON file, When I request the JSON schema for the export format, Then I should receive a valid JSON schema document that validates the exported JSON structure, includes version information for compatibility tracking, and is accessible via a documented URI or file location. Define schema in /infrastructure/schema.json using jsonschema==4.22.0 for validation.",
        "testStrategy": "Implement Behave scenario testing schema accessibility and validation capabilities. Verify schema validates exported JSON structure and includes proper versioning information for downstream tool compatibility.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing BDD test for schema access and validation",
            "description": "Create a Behave scenario testing that downstream developers can access a versioned JSON schema that validates exported data structure.",
            "status": "pending",
            "dependencies": [],
            "details": "Implement the BDD scenario: Given I have successfully exported project data to a JSON file, When I request the JSON schema for the export format, Then I should receive a valid JSON schema document, And the schema should validate the exported JSON structure, And the schema should include version information for compatibility tracking, And the schema should be accessible via a documented URI or file location.",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement 1: Make schema access scenario pass with versioning strategy",
            "description": "Implement JSON schema publishing with version information to make the BDD scenario pass, enabling downstream tool development.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create /infrastructure/schema.json with versioned schema that validates exported JSON structure. Ensure schema is accessible and includes compatibility tracking information for downstream tool developers.",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 13,
        "title": "Acceptance Test Suite with Behave",
        "description": "Develop comprehensive BDD acceptance test infrastructure using behave==1.2.6 to validate CLI functionality against live Azure DevOps environments. This technical infrastructure task supports validation of all user-facing functionality through automated testing.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12
        ],
        "priority": "high",
        "details": "Implement technical infrastructure for BDD acceptance testing using behave==1.2.6 that executes against ephemeral Azure DevOps organizations. Tests must validate real API interactions without mocks, provide detailed failure feedback, and support validation of all CLI requirements. Place features in /features directory and integrate with azdo-demo-org for ephemeral org provisioning.",
        "testStrategy": "Given the behave test infrastructure is implemented, When executing the test suite against ephemeral Azure DevOps organizations, Then all technical components should function correctly including org provisioning, API interactions, step definitions, and failure reporting mechanisms.",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup Behave Framework Structure",
            "description": "Initialize behave==1.2.6 framework with proper directory structure and configuration files.",
            "status": "pending",
            "dependencies": [],
            "details": "Create /features directory structure, configure behave.ini, and set up step definitions directory with proper Python imports.",
            "testStrategy": "Verify behave can discover and execute basic test scenarios; confirm directory structure follows behave conventions."
          },
          {
            "id": 2,
            "title": "Implement Core Step Definition Framework",
            "description": "Develop reusable step definition framework for CLI command execution, file operations, and API validation.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create base step definitions in /features/steps for common operations like CLI execution, JSON parsing, file validation, and error checking.",
            "testStrategy": "Unit test step definitions for correctness; ensure all common patterns are covered and reusable."
          },
          {
            "id": 3,
            "title": "Integrate Ephemeral Azure DevOps Org Provisioning",
            "description": "Implement azdo-demo-org integration for automatic provisioning and teardown of test organizations.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Configure before/after hooks in behave to create and destroy ephemeral Azure DevOps organizations for each test run, ensuring isolation.",
            "testStrategy": "Validate org creation and cleanup in test logs; ensure no residual resources remain after test execution."
          },
          {
            "id": 4,
            "title": "Implement Real API Interaction Testing",
            "description": "Configure test framework to execute against live Azure DevOps APIs without mocking for authentic validation.",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "Ensure all test scenarios make real API calls against ephemeral organizations, validating actual API responses and behavior.",
            "testStrategy": "Verify no mock libraries are used; confirm all API interactions are against live endpoints with real authentication."
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Failure Reporting",
            "description": "Implement detailed failure feedback mechanism that captures CLI output, API responses, and environment state.",
            "status": "pending",
            "dependencies": [
              4
            ],
            "details": "Enhance behave reporting with rich logging that captures full context on failures, including API response details and CLI command output.",
            "testStrategy": "Verify failure reports contain actionable debugging information including API responses, CLI output, and environment state."
          },
          {
            "id": 6,
            "title": "Create Feature Files for Core CLI Functions",
            "description": "Author Gherkin feature files that cover all CLI functionality including authentication, project export, and error handling.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Write .feature files in /features directory covering authentication flows, project metadata export, process behaviors, and error scenarios.",
            "testStrategy": "Review feature files for completeness; ensure all CLI functions have corresponding Gherkin scenarios."
          },
          {
            "id": 7,
            "title": "Integrate Test Suite with CI Pipeline",
            "description": "Configure CI integration to execute behave test suite against ephemeral organizations and publish results.",
            "status": "pending",
            "dependencies": [
              5,
              6
            ],
            "details": "Set up CI pipeline to run 'uv run behave' command, manage ephemeral org lifecycle, and publish test artifacts with detailed results.",
            "testStrategy": "Confirm test suite executes in CI; verify ephemeral org management works in automated environment; ensure test results are published."
          }
        ]
      },
      {
        "id": 14,
        "title": "Continuous Integration Setup",
        "description": "Configure automated CI pipeline to validate code quality, execute tests, and measure performance metrics for reliable code delivery.",
        "status": "pending",
        "dependencies": [
          13
        ],
        "priority": "medium",
        "details": "Implement GitHub Actions or Azure Pipelines CI configuration that runs: ruff linting for code quality, pytest for unit tests, Behave acceptance test suite, and performance checks for runtime (< 5 min) and file size (< 50 MB) metrics. Pipeline should block merges on any failures to maintain code quality gates.",
        "testStrategy": "Verify CI pipeline executes all validation steps: linting passes with zero violations, unit tests pass, Behave acceptance tests complete successfully, performance metrics meet targets, and merge protection prevents failing code from being merged.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for automated validation pipeline",
            "description": "Create a Behave scenario testing that the CI pipeline validates code quality, runs acceptance tests, and measures performance metrics, verifying it fails initially without CI configuration.",
            "status": "pending",
            "dependencies": [],
            "details": "Write BDD scenario covering: code linting validation, acceptance test execution against live Azure DevOps, performance metrics collection, artifact publishing, and merge blocking on failures.",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement 1: Configure CI pipeline workflow",
            "description": "Implement CI pipeline configuration (GitHub Actions or Azure Pipelines) to execute validation steps and enforce quality gates.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create CI configuration with jobs for: ruff linting, pytest unit tests, Behave acceptance test suite, runtime and file size performance checks, and merge protection rules. Ensure pipeline blocks merges on any step failures.",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 15,
        "title": "Documentation and CLI Help Verification",
        "description": "Implement comprehensive documentation and CLI help system infrastructure to support user onboarding and provide technical reference materials for the Azure DevOps export tool.",
        "status": "pending",
        "dependencies": [
          2,
          11,
          12,
          14
        ],
        "priority": "medium",
        "details": "Implement technical documentation infrastructure including CLI --help output verification against PRD specifications, README.md with usage examples and expected outputs, error code documentation with troubleshooting guidance, and authentication setup instructions. Validate all documented examples execute correctly.",
        "testStrategy": "BDD scenario: Given the CLI tool requires comprehensive documentation, When implementing help and reference systems, Then CLI --help should match PRD specifications exactly, README should provide clear usage examples with expected outputs, error codes should be documented with troubleshooting guidance, authentication setup should be clearly explained with examples, and all documented examples should execute successfully.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scenario 1: Write failing test for CLI help documentation",
            "description": "Create a Behave scenario testing CLI help documentation and verify it fails initially.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement 1: Make CLI help scenario pass and final refactor",
            "description": "Implement CLI help documentation to make scenario pass, then refactor for PRD compliance and production readiness.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement comprehensive README with technical usage examples",
            "description": "Create README.md with technical documentation including CLI usage patterns, expected JSON output formats, and authentication configuration examples.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Include working examples for all CLI commands, show expected JSON output formats, and provide step-by-step authentication setup with real examples.",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement error code documentation and troubleshooting reference",
            "description": "Create technical documentation for error codes with troubleshooting procedures for common failure scenarios.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Document all exit codes (0, 1, 2), common error scenarios, and provide actionable troubleshooting steps with examples.",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Validate documented examples execute correctly",
            "description": "Execute all CLI examples in documentation to verify technical accuracy and ensure they produce expected outputs.",
            "status": "pending",
            "dependencies": [
              3,
              4
            ],
            "details": "Execute every CLI example in README and help documentation to verify accuracy and update any that don't work as expected.",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 16,
        "title": "Fetch Work Item Types from Azure DevOps REST APIs",
        "description": "As a process analyst, I want to export work item type definitions from Azure DevOps so that I can understand the process configuration and analyze how different work item types are structured and used across projects.",
        "status": "pending",
        "dependencies": [
          5,
          3
        ],
        "priority": "high",
        "details": "Implement functionality to retrieve work item type definitions from Azure DevOps using the /wit/workitemtypes endpoint. Create a new module in /domain/work_item_types.py for domain objects representing work item types, fields, states, and rules. Implement the API client logic in /infrastructure/azdo_client.py using either azure-devops==7.1.0 library or direct REST calls via httpx. The implementation should fetch work item type definitions for a given project, including all associated metadata such as field definitions, workflow states, and business rules. Focus on enabling the CLI command 'azdo-process-export process \"Demo Project\" --out project.json' to successfully export work item type definitions as part of the complete project configuration. Serialize API responses into strongly-typed domain objects with proper validation and include comprehensive error handling for API failures, network issues, and malformed responses.",
        "testStrategy": "Write Behave scenarios in /features/work_item_types.feature to test the user-facing workflow: 'Given I have access to an Azure DevOps project with configured work item types And I have valid authentication credentials When I run \"azdo-process-export process 'Demo Project' --out project.json\" Then the output file should contain work item type definitions And each work item type should include name, fields, states, and rules And the structured logs should show work item type retrieval progress And the export should complete successfully even for projects with custom work item types'. Include additional scenarios for handling missing work item types, API error conditions, and authentication issues. Create unit tests for domain object serialization and validation.",
        "subtasks": [
          {
            "id": 1,
            "title": "Research: Work Item Types APIs",
            "description": "Research Azure DevOps REST APIs for work item types using Microsoft docs and azure-devops library documentation",
            "status": "pending",
            "dependencies": [],
            "details": "Use microsoft_docs_search and get-library-docs to understand work item types endpoints, response schemas, and best practices",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Plan: Work Item Types Implementation",
            "description": "Based on research findings, create RED->GREEN->REFACTOR subtasks for implementing work item types fetch",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "After completing research, break down the implementation into focused BDD/TDD cycles with single scenarios",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement BDD Scenario: Export Work Item Types",
            "description": "Implement the main user-facing BDD scenario for exporting work item type definitions",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Create the Behave scenario that tests the complete workflow from CLI command to JSON output containing work item type definitions with name, fields, states, and rules",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create Domain Objects for Work Item Types",
            "description": "Create /domain/work_item_types.py with domain objects for WorkItemType, Field, State, and Rule",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Design domain objects that properly represent Azure DevOps work item type schema and support the user's need to understand process configuration",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement API Client for Work Item Types",
            "description": "Add work item types fetching functionality to /infrastructure/azdo_client.py",
            "status": "pending",
            "dependencies": [
              4
            ],
            "details": "Implement methods to fetch work item type definitions using /wit/workitemtypes endpoint, with proper error handling and logging for user feedback",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Integrate Work Item Types into CLI Export",
            "description": "Integrate work item types fetching into the main CLI export command",
            "status": "pending",
            "dependencies": [
              5
            ],
            "details": "Ensure work item type definitions are included in the project.json output when users run the export command, with progress logging",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 17,
        "title": "Fetch Work Item Fields from Azure DevOps REST APIs",
        "description": "As a data analyst, I want to export field definitions from Azure DevOps so that I can understand the data schema and build accurate reports that use the correct field types and constraints.",
        "status": "pending",
        "dependencies": [
          5,
          3,
          4
        ],
        "priority": "high",
        "details": "Implement BDD-aligned functionality to retrieve comprehensive field definitions from Azure DevOps using the /wit/fields endpoint. The implementation should support the user scenario: exporting field metadata for data schema understanding and accurate reporting. Create domain objects in /domain/work_item_fields.py for FieldDefinition, FieldType, FieldConstraints, and AllowedValues classes. Implement API client logic in /infrastructure/azdo_client.py using httpx to fetch field definitions including data types, validation rules, allowed values for picklist fields, and constraints. Ensure custom fields are clearly distinguished from system fields. Include comprehensive field metadata retrieval with structured logging to show progress. The export should handle complex validation rules and be accessible via the main CLI command 'azdo-process-export process' with JSON output.",
        "testStrategy": "Primary BDD scenario: Given I have access to an Azure DevOps project with custom and system fields and valid authentication credentials, When I run 'azdo-process-export process 'Demo Project' --out project.json', Then the output file should contain comprehensive field definitions with name, type, constraints, and allowed values, custom fields should be clearly distinguished from system fields, structured logs should show field metadata retrieval progress, and the export should handle fields with complex validation rules correctly. Include additional scenarios for API error handling (401, 403, 404, 429, 500) and edge cases such as large allowed value lists and complex field constraints.",
        "subtasks": [
          {
            "id": 1,
            "title": "Research: Work Item Fields APIs",
            "description": "Research Azure DevOps REST APIs for work item fields using Microsoft docs and azure-devops library documentation",
            "status": "pending",
            "dependencies": [],
            "details": "Use microsoft_docs_search and get-library-docs to understand fields endpoints, field definitions, and retrieval best practices",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Plan: Work Item Fields Implementation",
            "description": "Based on research findings, create RED->GREEN->REFACTOR subtasks for implementing work item fields fetch",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "After completing research, break down the implementation into focused BDD/TDD cycles with single scenarios",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Write BDD Scenario for Field Schema Export",
            "description": "Create the primary BDD scenario for data analyst field schema export workflow",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Write the main scenario in /features/work_item_fields.feature: Given Azure DevOps project with custom and system fields, When running 'azdo-process-export process 'Demo Project' --out project.json', Then output contains comprehensive field definitions with proper structure and custom field distinction",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Field Definition Domain Objects",
            "description": "Create domain objects for field definitions optimized for data schema understanding",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "Implement FieldDefinition, FieldType, FieldConstraints, and AllowedValues classes in /domain/work_item_fields.py with clear distinction between custom and system fields, supporting the data analyst's need to understand schema for reporting",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Fields API Client Integration",
            "description": "Create API client logic to fetch field definitions via /wit/fields endpoint",
            "status": "pending",
            "dependencies": [
              4
            ],
            "details": "Implement field retrieval in /infrastructure/azdo_client.py using httpx, focusing on comprehensive metadata collection including complex validation rules and constraints needed for accurate data analysis",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Integrate Fields Export into Main CLI Command",
            "description": "Connect field definitions export to the main azdo-process-export command",
            "status": "pending",
            "dependencies": [
              5
            ],
            "details": "Ensure field definitions are included in the JSON output when running 'azdo-process-export process' command, with structured logging showing field metadata retrieval progress for user visibility",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 18,
        "title": "Fetch Process Behaviors from Azure DevOps REST APIs",
        "description": "As a process coach, I want to export process behavior configurations from Azure DevOps so that I can analyze workflow rules and state transitions to identify process optimization opportunities. Implement functionality to retrieve process behaviors using the /processes/{processId}/behaviors endpoint and export them in a format suitable for workflow analysis.",
        "status": "pending",
        "dependencies": [
          5,
          3,
          4
        ],
        "priority": "high",
        "details": "Implement user-focused process behavior export functionality that enables process coaches to analyze workflow optimization opportunities. Create domain objects in /domain/process_behaviors.py for ProcessBehavior, StateTransition, BusinessRule, and ProcessCustomization that clearly represent workflow analysis data. Implement API client logic in /infrastructure/azdo_client.py to fetch behavior configurations via /processes/{processId}/behaviors endpoint. The CLI command 'azdo-process-export process \"Demo Project\" --out project.json' should export process behaviors with state transitions, business rules, and customizations in a structured format that supports workflow analysis. Include clear documentation of state transition rules to enable process optimization analysis. Handle custom process templates correctly and provide structured logging to show behavior retrieval progress for user feedback.",
        "testStrategy": "Write primary BDD scenario in /features/process_behaviors.feature: Given access to Azure DevOps organization with configured process behaviors and valid authentication, When running 'azdo-process-export process \"Demo Project\" --out project.json', Then output file contains process behavior configurations with state transitions, business rules, and customizations, structured logs show retrieval progress, export handles custom process templates, and state transition rules are documented for analysis. Include error handling scenarios for invalid credentials, missing processes, and API failures with appropriate exit codes and user-friendly error messages.",
        "subtasks": [
          {
            "id": 1,
            "title": "Research: Process Behaviors APIs",
            "description": "Research Azure DevOps REST APIs for process behaviors using Microsoft docs and azure-devops library documentation",
            "status": "pending",
            "dependencies": [],
            "details": "Use microsoft_docs_search and get-library-docs to understand behaviors endpoints, process configurations, and retrieval patterns",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Plan: Process Behaviors Implementation",
            "description": "Based on research findings, create RED->GREEN->REFACTOR subtasks for implementing process behaviors fetch",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "After completing research, break down the implementation into focused BDD/TDD cycles with single scenarios",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "BDD Scenario: Process Coach Workflow Analysis Export",
            "description": "Implement primary BDD scenario for process coach exporting behavior configurations for workflow analysis",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Create /features/process_behaviors.feature with scenario: Given Azure DevOps org with process behaviors and valid auth, When running 'azdo-process-export process \"Demo Project\" --out project.json', Then output contains behaviors with state transitions/rules/customizations, logs show progress, handles custom templates, and rules documented for analysis",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Domain Objects: Workflow Analysis Focus",
            "description": "Create process behavior domain objects optimized for workflow analysis and process optimization",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "Implement ProcessBehavior, StateTransition, BusinessRule, and ProcessCustomization classes in /domain/process_behaviors.py with clear workflow analysis properties and methods that support process coach needs for identifying optimization opportunities",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "CLI Integration: Process Coach Command",
            "description": "Integrate process behavior export into CLI with user-friendly output for process coaches",
            "status": "pending",
            "dependencies": [
              4
            ],
            "details": "Extend CLI to support process behavior export with structured output suitable for workflow analysis, including clear state transition documentation and process optimization insights",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 19,
        "title": "Fetch Teams from Azure DevOps REST APIs",
        "description": "As a team lead, I want to export team information from Azure DevOps so that I can understand team structure, member roles, and configuration settings for organizational planning and team optimization.",
        "status": "pending",
        "dependencies": [
          5,
          3
        ],
        "priority": "high",
        "details": "Implement end-to-end functionality to make this BDD scenario pass: Given I have access to an Azure DevOps project with configured teams and valid authentication credentials, When I run 'azdo-process-export process \"Demo Project\" --out project.json', Then the output file should contain comprehensive team information, each team should include member lists with roles and configuration settings, team settings like backlog navigation and working days should be captured, the structured logs should show team data retrieval progress, and the export should handle teams with large member lists efficiently. Create domain objects in /domain/teams.py for Team, TeamMember, TeamSettings, and TeamConfiguration classes. Implement API client logic in /infrastructure/azdo_client.py using the /core/teams endpoint. Focus on user value: enabling organizational planning through understanding team structure rather than just technical API implementation.",
        "testStrategy": "Single BDD scenario testing the complete user workflow: 'Export team information for organizational planning'. Given access to Azure DevOps project with configured teams and valid credentials, When running the export command, Then verify the output contains comprehensive team data including member lists, roles, configuration settings, team settings like backlog navigation and working days, structured logs showing progress, and efficient handling of large member lists. Include edge case scenarios for teams without members, teams with complex configurations, and authentication/permission issues.",
        "subtasks": [
          {
            "id": 1,
            "title": "Research: Teams APIs",
            "description": "Research Azure DevOps REST APIs for teams using Microsoft docs and azure-devops library documentation",
            "status": "pending",
            "dependencies": [],
            "details": "Use microsoft_docs_search and get-library-docs to understand teams endpoints, team configurations, and member retrieval patterns",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Plan: Teams Implementation",
            "description": "Based on research findings, create RED->GREEN->REFACTOR subtasks for implementing teams fetch",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "After completing research, break down the implementation into focused BDD/TDD cycles with single scenarios",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 20,
        "title": "Fetch Backlogs from Azure DevOps REST APIs",
        "description": "As a scrum master, I want to export backlog configurations and team settings from Azure DevOps so that I can analyze sprint planning efficiency and optimize iteration management across multiple teams.",
        "status": "pending",
        "dependencies": [
          5,
          3,
          19
        ],
        "priority": "high",
        "details": "Implement BDD-driven functionality to retrieve backlog configurations, iterations, and team settings that enables sprint planning analysis. The feature should support the CLI command 'azdo-process-export process \"Demo Project\" --out project.json' and export comprehensive backlog data including hierarchy levels (Epic, Feature, User Story, etc.), iteration paths and dates, team capacity settings, and working days configuration. Create domain objects in /domain/backlogs.py for Backlog, BacklogConfiguration, Iteration, TeamSettings, and BacklogLevel classes. Implement API client logic in /infrastructure/azdo_client.py using /work/backlogs and /work/teamsettings endpoints. The export should handle complex backlog hierarchies and provide structured logging for data retrieval progress. Focus on delivering value for sprint planning efficiency analysis rather than just technical API integration.",
        "testStrategy": "Write a primary BDD scenario in /features/backlogs.feature: Given access to Azure DevOps project with configured backlogs and team settings, When running 'azdo-process-export process \"Demo Project\" --out project.json', Then output should contain backlog configurations and iteration settings with team hierarchy levels, iteration paths, working days, and capacity settings. Include structured logs showing backlog data retrieval progress and proper handling of complex backlog hierarchies. Add supporting scenarios for authentication failures, missing resources, and API errors. Focus on user value validation rather than just technical API testing.",
        "subtasks": [
          {
            "id": 1,
            "title": "Research: Backlogs APIs",
            "description": "Research Azure DevOps REST APIs for backlogs using Microsoft docs and azure-devops library documentation",
            "status": "pending",
            "dependencies": [],
            "details": "Use microsoft_docs_search and get-library-docs to understand backlogs endpoints, backlog configurations, and work item retrieval patterns",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Plan: Backlogs Implementation",
            "description": "Based on research findings, create RED->GREEN->REFACTOR subtasks for implementing backlogs fetch",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "After completing research, break down the implementation into focused BDD/TDD cycles with single scenarios",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "BDD Scenario: Sprint Planning Analysis Export",
            "description": "Write the primary BDD scenario for scrum master backlog export use case",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "Create Behave scenario in /features/backlogs.feature that validates the complete user journey: running CLI command to export backlog configurations for sprint planning efficiency analysis, including backlog hierarchies, iteration settings, team capacity, and working days",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Domain Objects: Backlog Configuration Models",
            "description": "Implement domain objects focused on sprint planning analysis data",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "Create /domain/backlogs.py with Backlog, BacklogConfiguration, Iteration, TeamSettings, and BacklogLevel classes optimized for capturing sprint planning efficiency data and iteration management insights",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Infrastructure: Backlog API Integration",
            "description": "Implement API client for /work/backlogs and /work/teamsettings endpoints",
            "status": "pending",
            "dependencies": [
              4
            ],
            "details": "Build API integration in /infrastructure/azdo_client.py that retrieves comprehensive backlog data for sprint planning analysis, including complex hierarchy handling and structured logging for data retrieval progress",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-07-18T20:14:51.299Z",
      "updated": "2025-07-21T00:37:41.043Z",
      "description": "Tasks for master context"
    }
  }
}